{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "composite-adolescent",
   "metadata": {},
   "source": [
    "# Reinforcement - Syft Duet - Data Scientist ü•Å\n",
    "\n",
    "Contributed by [@Koukyosyumei](https://github.com/Koukyosyumei)\n",
    "\n",
    "This example trains a CartPole Reinforcement network with Gym over Syft.\n",
    "This notebook is mainly based on the original pytorch [example](https://github.com/OpenMined/PySyft/tree/dev/examples/duet/reinforcement_learning/original)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divine-national",
   "metadata": {},
   "source": [
    "## PART 1: Connect to a Remote Duet Server\n",
    "\n",
    "As the Data Scientist, you want to perform data science on data that is sitting in the Data Owner's Duet server in their Notebook.\n",
    "\n",
    "In order to do this, we must run the code that the Data Owner sends us, which importantly includes their Duet Session ID. The code will look like this, importantly with their real Server ID.\n",
    "\n",
    "```\n",
    "import syft as sy\n",
    "duet = sy.duet('xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx')\n",
    "```\n",
    "\n",
    "This will create a direct connection from my notebook to the remote Duet server. Once the connection is established all traffic is sent directly between the two nodes.\n",
    "\n",
    "Paste the code or Server ID that the Data Owner gives you and run it in the cell below. It will return your Client ID which you must send to the Data Owner to enter into Duet so it can pair your notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-emission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdlib\n",
    "import os\n",
    "import time\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "\n",
    "# third party\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "loop = asyncio.get_event_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-avenue",
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "\n",
    "duet = sy.join_duet(loopback=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-train",
   "metadata": {},
   "source": [
    "### <img src=\"https://github.com/OpenMined/design-assets/raw/master/logos/OM/mark-primary-light.png\" alt=\"he-black-box\" width=\"100\"/> Checkpoint 0 : Now STOP and run the Data Owner notebook until Checkpoint 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-toddler",
   "metadata": {},
   "source": [
    "## PART 2: Check GPU, Define a model, optimizer, and other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-guidance",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"gamma\": 0.99,\n",
    "    \"seed\": 543,\n",
    "    \"render\": False,\n",
    "    \"log_interval\": 10,\n",
    "    \"no_cuda\": False,\n",
    "    \"log_interval\": 1,\n",
    "    \"wait_interval\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-sunrise",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_torch = duet.torch\n",
    "remote_torch.manual_seed(config[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-disability",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_cuda = False\n",
    "has_cuda_ptr = remote_torch.cuda.is_available()\n",
    "\n",
    "# lets ask to see if our Data Owner has CUDA\n",
    "has_cuda = bool(\n",
    "    has_cuda_ptr.get(\n",
    "        request_block=True,\n",
    "        reason=\"To run test and inference locally\",\n",
    "        timeout_secs=3,  # change to something slower\n",
    "    )\n",
    ")\n",
    "print(\"Is cuda available ? : \", has_cuda)\n",
    "\n",
    "\n",
    "use_cuda = not config[\"no_cuda\"] and has_cuda\n",
    "# now we can set the seed\n",
    "remote_torch.manual_seed(config[\"seed\"])\n",
    "\n",
    "device = remote_torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "# print(f\"Data Owner device is {device.type.get()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-visibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(sy.Module):\n",
    "    def __init__(self, torch_ref):\n",
    "        super(Policy, self).__init__(torch_ref=torch_ref)\n",
    "        self.affine1 = self.torch_ref.nn.Linear(4, 128)\n",
    "        self.dropout = self.torch_ref.nn.Dropout(p=0.6)\n",
    "        self.relu = self.torch_ref.nn.ReLU(True)\n",
    "        self.affine2 = self.torch_ref.nn.Linear(128, 2)\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.affine1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        action_scores = self.affine2(x)\n",
    "        return remote_torch.softmax(action_scores, dim=1)\n",
    "\n",
    "\n",
    "# You cannot see the state\n",
    "def select_action(state):\n",
    "    probs_ptr = remote_policy(state)\n",
    "    probs = probs_ptr.get(request_block=True, delete_obj=False)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    remote_policy.saved_log_probs.append(m.log_prob(action))\n",
    "    return action.item()\n",
    "\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    returns = []\n",
    "    for r in remote_policy.rewards[::-1]:\n",
    "        R = r + config[\"gamma\"] * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    for log_prob, R in zip(remote_policy.saved_log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del remote_policy.rewards[:]\n",
    "    del remote_policy.saved_log_probs[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-dependence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send our model to remote\n",
    "policy = Policy(torch)\n",
    "remote_policy = policy.send(duet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assured-cleaner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we have CUDA lets send our model to the GPU\n",
    "if has_cuda:\n",
    "    remote_policy.cuda(device)\n",
    "else:\n",
    "    remote_policy.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reverse-chapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = remote_torch.optim.Adam(remote_policy.parameters(), lr=1e-2)\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-miller",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_threshold_ptr = duet.store[\"reward_threshold\"]\n",
    "reward_threshold = reward_threshold_ptr.get(request_block=True, delete_obj=False)\n",
    "print(f\"reward_threshold is {reward_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-controversy",
   "metadata": {},
   "outputs": [],
   "source": [
    "duet.store.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-involvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_folder = \"./nb_checkpoints\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-edinburgh",
   "metadata": {},
   "source": [
    "### <img src=\"https://github.com/OpenMined/design-assets/raw/master/logos/OM/mark-primary-light.png\" alt=\"he-black-box\" width=\"100\"/> Checkpoint 1 : Now STOP and run the Data Owner notebook until Checkpoint 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-simulation",
   "metadata": {},
   "source": [
    "## PART 3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-subject",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "running_reward = 10\n",
    "episodes = 10\n",
    "steps = 30\n",
    "\n",
    "for i_episode in range(episodes):\n",
    "    ep_reward = 0\n",
    "\n",
    "    # 10000\n",
    "    for t in range(1, steps):\n",
    "\n",
    "        # wait for data owner to send state\n",
    "        for retry in range(360):\n",
    "            if Path(f\"{checkpoints_folder}/DO_checkpoint_state_{count}\").exists():\n",
    "                break\n",
    "            task = loop.create_task(asyncio.sleep(config[\"wait_interval\"]))\n",
    "            loop.run_until_complete(task)\n",
    "        assert Path(f\"{checkpoints_folder}/DO_checkpoint_state_{count}\").exists()\n",
    "        # get state from data owner\n",
    "        state = duet.store[f\"state_{count}\"]\n",
    "\n",
    "        action = select_action(state)\n",
    "        # send action to data owner\n",
    "        sy_action = sy.lib.python.Int(action)\n",
    "        sy_action.tag(f\"action_{count}\")\n",
    "        sy_action.send(duet)\n",
    "        Path(f\"{checkpoints_folder}/DS_checkpoint_action_{count}\").touch()\n",
    "\n",
    "        # wait for data owner to send reward\n",
    "        for retry in range(360):\n",
    "            if Path(f\"{checkpoints_folder}/DO_checkpoint_reward_{count}\").exists():\n",
    "                break\n",
    "            task = loop.create_task(asyncio.sleep(config[\"wait_interval\"]))\n",
    "            loop.run_until_complete(task)\n",
    "        assert Path(f\"{checkpoints_folder}/DO_checkpoint_reward_{count}\").exists()\n",
    "        # get reward from data owner\n",
    "        reward_ptr = duet.store[f\"reward_{count}\"]\n",
    "        reward = reward_ptr.get(request_block=True, delete_obj=False)\n",
    "        remote_policy.rewards.append(reward)\n",
    "        ep_reward += reward\n",
    "\n",
    "        # wait for data owner to send done\n",
    "        for retry in range(360):\n",
    "            if Path(f\"{checkpoints_folder}/DO_checkpoint_done_{count}\").exists():\n",
    "                break\n",
    "            task = loop.create_task(asyncio.sleep(config[\"wait_interval\"]))\n",
    "            loop.run_until_complete(task)\n",
    "        assert Path(f\"{checkpoints_folder}/DO_checkpoint_done_{count}\").exists()\n",
    "        # get done from data owner\n",
    "        done_ptr = duet.store[f\"done_{count}\"]\n",
    "        done = done_ptr.get(request_block=True, delete_obj=False)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "    finish_episode()\n",
    "    if i_episode % config[\"log_interval\"] == 0:\n",
    "        print(\n",
    "            \"Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}\".format(\n",
    "                i_episode, ep_reward, running_reward\n",
    "            )\n",
    "        )\n",
    "    if running_reward > reward_threshold:\n",
    "        print(\n",
    "            \"Solved! Running reward is now {} and \"\n",
    "            \"the last episode runs to {} time steps!\".format(running_reward, t)\n",
    "        )\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-emperor",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_policy = remote_policy.get(request_block=True, timeout_secs=5)\n",
    "local_policy.save(\"./cartpole.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-major",
   "metadata": {},
   "source": [
    "### <img src=\"https://github.com/OpenMined/design-assets/raw/master/logos/OM/mark-primary-light.png\" alt=\"he-black-box\" width=\"100\"/> Checkpoint 2 : Now STOP and run the Data Owner notebook until the next checkpoint."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
